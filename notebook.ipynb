{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0ad1ef-52f2-494c-a3ca-cb52e899928e",
   "metadata": {},
   "source": [
    "# Electricity Price Prediction | ML Zoomcamp Cap Stone #1 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801661a-f864-4af8-8239-3409947d6d0d",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e150a-d3f9-46cf-b610-e94c0709af50",
   "metadata": {},
   "source": [
    "### Library import and dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbab570-36fa-4cae-a6f9-b306e58b2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b230c-5dac-4c40-8c95-d91b6b20597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data = pd.read_csv(r\"data/energy_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab48ca-9530-4270-a71d-2354bab821fa",
   "metadata": {},
   "source": [
    "### Initial peak at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3d65e-b56d-428c-ac77-0b5fd6a3558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be10bf7-4424-4b10-aa6e-db1c5e92203a",
   "metadata": {},
   "source": [
    "Ok, so a lot of columns (29 in total) almost all of them numeric and I would group them in 5 major groups:\n",
    "* time - it reads like an object so we should cast it to pandas datetime\n",
    "* power generation by the different types of energy source\n",
    "  * some of them seem to not hold any value and thus should be discarded and I also wonder about non-null values fluctuations between them - maybe not all of them are used\n",
    "* renewable energy forecasts (solar and wind on and off shore)\n",
    "  * as we have the weather data in the other dataset we might want to drop these\n",
    "* power demand (aka load) actual and forecasted at each datapoint (hour)\n",
    "* prices for each moment (actual - our target variable - and for day ahead)\n",
    "\n",
    "Let's see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9507439-e9d4-4ab1-8bc3-276cb8618115",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f123b24-10cd-4147-9c26-d15252592b91",
   "metadata": {},
   "source": [
    "Ok, interesting, so it seems like a lot of the generation sources do not have any values other than 0 or null - it makes sense as the country in question (Spain) does not have all the types of energy generated, namely: ```generation fossil coal-derived gas```, ```ggeneration fossil oil shale```, ```generation fossil peat```, ```generation geothermal```, ```generation marine```, ```generation wind offshore``` and ```generation hydro pumped storage aggregated```.\n",
    "\n",
    "For all the other their values vary between 0 and whatever their peak is and load and prices never reach 0, which makes sense has the country has always demand for power.\n",
    "\n",
    "Going forward I will drop all the columns stated above that are irrevelant and let's see how their chart looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd3323-611c-4c81-afbf-ec5dc9174cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_columns = [\n",
    "    'generation biomass', 'generation fossil brown coal/lignite',\n",
    "    'generation fossil gas', 'generation fossil hard coal', \n",
    "    'generation fossil oil',\n",
    "    'generation hydro pumped storage consumption',\n",
    "    'generation hydro run-of-river and poundage',\n",
    "    'generation hydro water reservoir',\n",
    "    'generation nuclear', 'generation other', 'generation other renewable',\n",
    "    'generation solar', 'generation waste',\n",
    "    'generation wind onshore']\n",
    "\n",
    "forecast_columns = ['forecast solar day ahead','forecast wind onshore day ahead']\n",
    "\n",
    "demand_columns = ['total load actual', 'total load forecast']\n",
    "\n",
    "price_columns = ['price day ahead', 'price actual']\n",
    "\n",
    "energy_data['time'] = pd.to_datetime(energy_data['time'], utc=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a80f48-a568-4341-a14c-f72deedddd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_color = 'blue'\n",
    "forecast_color = 'orange'\n",
    "demand_color = 'green'\n",
    "price_color = 'red'\n",
    "\n",
    "column_color_mapping = {col: generation_color for col in generation_columns}\n",
    "column_color_mapping.update({col: forecast_color for col in forecast_columns})\n",
    "column_color_mapping.update({col: demand_color for col in demand_columns})\n",
    "column_color_mapping.update({col: price_color for col in price_columns})\n",
    "\n",
    "all_columns = generation_columns + forecast_columns + demand_columns + price_columns\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "num_rows = 7\n",
    "num_cols = 3\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(all_columns):\n",
    "    ax = axes[i]\n",
    "    ax.plot(energy_data['time'], energy_data[col], label=col, color=column_color_mapping[col])\n",
    "    ax.set_title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f90999-1788-4641-8732-22602941edb0",
   "metadata": {},
   "source": [
    "Look at those nice colours! Let's see what they are telling us:\n",
    "#### Generation (blue)\n",
    "* Each generation form seems to follow its own pattern - some, like ```other renewable``` or ```waste```, show a growing trend whereas ```biomass``` or ```oil``` have a clear decline halfway thru 2016 and ```hard coal``` also seems to be declining.\n",
    "* Renewable energy like ```wind onshore```, ```solar``` and ```hydro``` show clear signs of seasonality - which is natural for these sources.\n",
    "* In contrast to the ones above, due to its nature ```nuclear``` seems pretty stationary as well as ```gas```.\n",
    "* Some sources seem to compensate for other - e.g.: ```fossil gas``` seems to have a peak period in early 2017 when ```solar``` seems to be quite low which means there are some relations between them.\n",
    "* The sudden drops observed in some sources seems plausible given the generation cycle during the days / seasons. We should just need to take a look to ensure these are not because of the null values an correct for those - let's think about that later.\n",
    "#### Renewable Forecasts (orange)\n",
    "* They seem pretty aligned with the actual generation for the corresponding source, so most likely they should be taken out of the training data as they are redundant.\n",
    "#### Load (green)\n",
    "* Forecast and actual load show pretty similar pattern, so probably the forecast is something that could be made redundant as well.\n",
    "#### Prices (red)\n",
    "* Price day ahead shows a lot more spikes than the price actual - most likely I also won't want to use that one.\n",
    "* Price pattern and fluctuations seem quite different before 2016 and after 2016 - probably related with the generation mix changes originated from ```coal``` and ```oil``` around that period.\n",
    "* Interesting to observe that price peaks in early 2017 and 2018 also seem to correspond with peaks in ```gas``` related generation.\n",
    "\n",
    "In sum:\n",
    "* We need to prep the data for nulls\n",
    "* We need to deal with seasonal and non-stationary series - meaning we are going the scale the data\n",
    "* We might want to remove some redundant columns\n",
    "* It might be worth to discard past data (e.g: before 2016) as the nature of the generation mix might not be relevant going into the future and also to make the computation lighter.\n",
    " \n",
    "Let's continue and address these issues starting with this last one about the relevance of past data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c6363-c3db-4038-80fb-ac25fa64664c",
   "metadata": {},
   "source": [
    "### Structure of generation mix before and after 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf99f8d-f045-41b6-8cd7-5245a61b8cd1",
   "metadata": {},
   "source": [
    "Let's start by looking at the generation mix evolution on a monthly basis as we saw from the charts above that hourly data is too noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0dbb3-8c7e-4b39-8eb4-52499016f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_columns = generation_columns + ['month','year']\n",
    "energy_data['month'] = energy_data['time'].dt.month\n",
    "energy_data['year'] = energy_data['time'].dt.year\n",
    "grouped_df = energy_data[grouped_columns].groupby(['year', 'month']).sum().reset_index()\n",
    "grouped_df['date'] = pd.to_datetime(grouped_df[['year', 'month']].assign(day=28))\n",
    "grouped_df = grouped_df.drop(['year', 'month'], axis=1)\n",
    "\n",
    "# Create a custom color palette for generation source visualization\n",
    "brown_palette = sns.color_palette(['#5A2C0A', '#9E6732', '#A9C2A4', '#81400E'])\n",
    "blue_palette = sns.color_palette(['#1E4788', '#4A7FAD', '#6E98C6'])\n",
    "yellow_palette = sns.color_palette(['#CCB200'])\n",
    "gray_palette = sns.color_palette(['#737373', '#333333', '#5E5E5E'])\n",
    "orange_palette = sns.color_palette(['#8C3809'])\n",
    "green_palette = sns.color_palette(['#1E3D1A'])\n",
    "\n",
    "# Map each column to its corresponding color based on the specified groups\n",
    "column_color_mapping = {\n",
    "    'generation fossil brown coal/lignite': brown_palette[0],\n",
    "    'generation fossil gas': brown_palette[1],\n",
    "    'generation fossil hard coal': brown_palette[2],\n",
    "    'generation fossil oil': brown_palette[3],\n",
    "    'generation hydro pumped storage consumption': blue_palette[0],\n",
    "    'generation hydro run-of-river and poundage': blue_palette[1],\n",
    "    'generation hydro water reservoir': blue_palette[2],\n",
    "    'generation nuclear': yellow_palette[0],\n",
    "    'generation other': gray_palette[0],\n",
    "    'generation other renewable': gray_palette[1],\n",
    "    'generation biomass': gray_palette[1],\n",
    "    'generation waste': gray_palette[2],\n",
    "    'generation solar': orange_palette[0],\n",
    "    'generation wind onshore': green_palette[0]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.stackplot(\n",
    "    grouped_df['date'],\n",
    "    [grouped_df[col] for col in generation_columns],\n",
    "    labels=generation_columns,\n",
    "    colors=[column_color_mapping[col] for col in generation_columns],\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Add a red vertical dashed line at the beginning of 2016\n",
    "plt.axvline(pd.Timestamp('2016-01-01'), color='red', linestyle='--', linewidth=2, label='2016 Start')\n",
    "\n",
    "plt.title('Generation Mix Evolution')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Energy produced')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff1408-079f-4a4b-8833-250269e2d686",
   "metadata": {},
   "source": [
    "Unfortunately, even the monthly data seems messy and although it definitly looks like coal had a higher weight before 2016, it is not 100% clear from the chart above. Also we are not seeing how it affected our target variable - price, so let's try something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4caa9-c774-43ae-942a-b7b9922726c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sources = ['generation fossil hard coal', 'generation nuclear', 'generation wind onshore', \n",
    "                     'generation fossil gas', 'generation hydro water reservoir', 'generation solar']\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "selected_generation = energy_data[['time', 'price actual'] + top_sources].copy()\n",
    "\n",
    "# Calculate the sum of all other sources and add a new column 'All Other'\n",
    "selected_generation['All Other'] = energy_data[generation_columns].sum(axis=1) - selected_generation[top_sources].sum(axis=1)\n",
    "\n",
    "# Extract year from the 'time' column\n",
    "selected_generation['year'] = selected_generation['time'].dt.year\n",
    "\n",
    "# Create subplots for the generation mix and price distribution\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Stacked bar chart for the generation mix with percentage labels\n",
    "generation_data = selected_generation.groupby('year')[top_sources + ['All Other']].sum()\n",
    "generation_data_percentage = generation_data.div(generation_data.sum(axis=1), axis=0) * 100\n",
    "generation_data_percentage.plot(kind='bar', stacked=True, colormap='viridis', ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Mix evolution over the years')\n",
    "axs[0].set_xlabel('Year')\n",
    "axs[0].set_ylabel('Generation Mix (%)')\n",
    "axs[0].legend(title='Generation Source', loc='upper left', ncol=1)\n",
    "\n",
    "# Add percentage labels to the stacked bar chart\n",
    "for i in range(generation_data_percentage.shape[0]):\n",
    "    total = 0\n",
    "    for j, value in enumerate(generation_data_percentage.iloc[i]):\n",
    "        axs[0].text(i, total + value / 2, f'{value:.1f}%', ha='center', va='center', color='white')\n",
    "        total += value\n",
    "\n",
    "# Box plot for the price distribution\n",
    "sns.boxplot(x='year', y='price actual', data=selected_generation, palette='viridis', ax=axs[1])\n",
    "axs[1].set_title('Price Distribution Over Years')\n",
    "axs[1].set_xlabel('Year')\n",
    "axs[1].set_ylabel('Price Actual')\n",
    "\n",
    "medians = selected_generation.groupby('year')['price actual'].median()\n",
    "for i, median_value in enumerate(medians):\n",
    "    axs[1].text(i, median_value, f'{median_value:.2f}€/MWh', ha='center', va='bottom', color='white')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f355c-8831-4d32-8ba1-501b2040abb0",
   "metadata": {},
   "source": [
    "Ok, perfect so we clearly see that coal had a much higher weight in the generation mix before 2016 (~18% vs. ~13%)which reflected on a different price structure. In order to have the model not being too influenced from this past reality (that shouldn't repeat itself going forward as probably some coal plants have been decomissioned already) we should start our adventure from 2016 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ca2d4-7901-46cd-ba21-939caf842792",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data = energy_data[energy_data['time'] >= '2016-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9044e-1bb5-4438-bd53-e450f3b97bbc",
   "metadata": {},
   "source": [
    "### Handling Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd446e4-1490-4221-b91e-73143e02d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data[generation_columns + forecast_columns + demand_columns + price_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3135ae5-32a5-4036-80ce-8597daad9601",
   "metadata": {},
   "source": [
    "Given that ```total load actual``` is the one of the variables with most missing values let's have a detailed look at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e35a32-7d22-4cde-afb7-bbf492dfea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data[energy_data['total load actual'].isnull()].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff66370-ecf8-4c56-bef4-d1a3a25607af",
   "metadata": {},
   "source": [
    "Ok, so we have some nulls just at the beginning of the dataset so let's plot the first two weeks of data to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491a0b0-6126-485b-906f-44dee770e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_period_start_date = '2016-04-25'\n",
    "null_period_end_date = '2016-05-11'\n",
    "filtered_data = energy_data[(energy_data['time'] >= null_period_start_date) & (energy_data['time'] < null_period_end_date)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.plot(filtered_data['total load actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049e39d-0fdf-4441-9de2-eedb4aa48fe8",
   "metadata": {},
   "source": [
    "It seems like some hours are missing. To try to respect the original shape of the data, let's try to fill-in the missing values with the linear interpolation of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090a5fb-6ac7-456d-9752-6cc3bc6dffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e54157-4861-4fb2-8721-e0c401429ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_period_start_date = '2016-04-25'\n",
    "null_period_end_date = '2016-05-11'\n",
    "filtered_data = energy_data[(energy_data['time'] >= null_period_start_date) & (energy_data['time'] < null_period_end_date)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.plot(filtered_data['total load actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72122c2-b039-47f8-b2e5-baa419b83976",
   "metadata": {},
   "source": [
    "Ok, seems good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1022b2-7b7f-4bda-90ab-67eda046cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data[generation_columns + forecast_columns + demand_columns + price_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389de4e0-f877-4465-9968-b2d581bb2d60",
   "metadata": {},
   "source": [
    "### Skewness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b198260-0528-471f-837b-5255f2356506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "numerical = generation_columns + forecast_columns + demand_columns + price_columns\n",
    "fig, axs = plt.subplots(5, 4, figsize=(11,10))\n",
    "for col, ax in zip(energy_data[numerical].columns, axs.ravel()):\n",
    "    if energy_data[numerical][col].dtype == float:\n",
    "        ax.hist(energy_data[numerical][col], bins=100, color='red')\n",
    "    else: #int\n",
    "        vc = energy_data[numerical][col].value_counts()\n",
    "        ax.bar(vc.index, vc, color='red')\n",
    "    ax.set_xlabel('\\n'.join(wrap(col,25)), fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature distributions', y=1.02, fontsize=16, fontweight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7785db-8f1c-4dd2-ab06-591527d3f770",
   "metadata": {},
   "source": [
    "Ok, so let's break this down:\n",
    "* ```price actual``` shows a normal distribution, which is great for our goal\n",
    "* ```price day ahead```, ```total load actual```, ```total load forecast```, ```generation biomass```, ```generation fossil hard coal```, ```generation fossil oil``` all also show some kind of normal distribution.\n",
    "* Renewable energies (```wind onshore```, ```hydro``` and ```solar```) all show some skeweness to the right, excep ```generation other renewable```.\n",
    "* ```generation nuclear``` is skewed to the right and ```generation fossil gas``` is skewed to the left.\n",
    "\n",
    "Despite the skeweness in some input variables, as we will need to scale the values anyway to deal with the seasonality and trends I have opted not to do further transformations to the data here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2adac-cef7-437a-9f49-60f2dd13a721",
   "metadata": {},
   "source": [
    "### Feature correlation / importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf7a23-cf61-415b-ba01-a0f3137309b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = energy_data[numerical].corr(method = \"spearman\")\n",
    "plt.figure(figsize = (10, 7))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr, mask=mask, cmap = \"magma\", annot = True, annot_kws = {'size': 7})\n",
    "plt.title(\"Dataset correlation matrix\", fontsize = 12, fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf9680-c7b5-496c-871d-f014399144c6",
   "metadata": {},
   "source": [
    "Except for ```price day ahead``` we don't seem to have a very strong correlation between neither of the variables and our target (```price day actual```).\n",
    "\n",
    "Without surprise however, we can observe that ```generation fossil gas``` and ```generation fossil hard coal``` alongside with ```load``` variables are the ones that most relate to the price, as the amount of fossil fuels used and the demand has a direct impact on the price.\n",
    "\n",
    "Interesting to observe also that renewable forecasts have a slightly higher correlation with the target variable than the actual generation variables (e.g.: ```generation solar``` is just 0.13 whereas ```forecast solar day ahead``` is just 0.14). As such we will leave both these variables in the model.\n",
    "\n",
    "Also seems like whenever ```hydro``` is pumped it has a significant effect on price (more than just normal ```hydro``` generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6293c3f-78b0-428f-ae2f-fd9e2adfb509",
   "metadata": {},
   "source": [
    "#### Auto-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d3eab-f9d2-47c1-b020-41a9124db530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(energy_data['price actual'].values)\n",
    "axes[0, 0].set_title('Original Series')\n",
    "plot_acf(energy_data['price actual'], ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Autocorrelation Function')\n",
    "\n",
    "axes[1, 0].plot(energy_data['price actual'].values)\n",
    "axes[1, 0].set_title('Original Series')\n",
    "plot_pacf(energy_data['price actual'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Partial Autocorrelation Function')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fc6cf-8892-48d7-87b0-5d5134bea745",
   "metadata": {},
   "source": [
    "As it is normal in timeseries, we can definitly see some interesting auto and partial autocorrelations within our target value. This tells us that generating lag features (i.e.: shifting backwards the a previous value of the series observed at a specific time point) should help our model.\n",
    "\n",
    "According to these results 1h, 3h, 6h, 12 and 24h look like interesting lag features to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca15f8-a695-4ee2-9db2-006a67a5b5d1",
   "metadata": {},
   "source": [
    "### Additional EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022b80-a620-4448-bc77-49b661df9612",
   "metadata": {},
   "source": [
    "In addition to the points showed above, I've also ran a ```ProfileReport``` on the target data by the use of the following code that generated the profile_report.html file made available in this repo.\n",
    "\n",
    "```\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(energy_data, tsmode=True, sortby=\"time\")\n",
    "profile.to_file('profile_report.html')\n",
    "```\n",
    "\n",
    "The reason why I don't include this code in the notebook is because I had some troubles installing the ```ProfileReport``` module and don't want the reviewer colleagues going thru the same hassle.\n",
    "\n",
    "You can refer to the report result to understand better seasonality and estationarity properties of the data as well as many other details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8f468-c1a4-42b7-a571-1d8ab4da2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ydata_profiling import ProfileReport\n",
    "#\n",
    "#profile = ProfileReport(energy_data[numerical + ['time_str']], tsmode=True, sortby=\"time_str\")\n",
    "#profile.to_file('EDA_profile_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2273a4-1715-415f-b37a-3693f519ecbf",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "To wrap-up the data exploration I summarize below the main findings to take into consideration going forward:\n",
    "* We will train the model on periods after 2016 due to different generation mix.\n",
    "* We will fill-in null values with intrapolations.\n",
    "* We will include all columns except for the ones that are mostly empy.\n",
    "* We will create lag variables for 1h, 3h, 6h, 12h and 24h.\n",
    "* We will need to scale the values to adjust for seasonality and trends.\n",
    "* We know some features are skewed, but we will let it be for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1e157-87e4-4a5c-8cbb-960513016daa",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb30dd-a51a-4e9d-8db4-e8308cd70816",
   "metadata": {},
   "source": [
    "So the purpose of the current exercise is to predict with 1h in advance the price of electricity given the information available at the moment, i.e.:\n",
    "* the generation mix - how much is generation source is producing\n",
    "* the demand for electricity (and the estimated demand for the next day)\n",
    "* the history of the price (and the day ahead price as given by the market)\n",
    "\n",
    "This might not be most practical of the applications as some of this data is not made available in real time, but it helps us establish the predictive power of these variables alongside with a Neural Network that can then be expaneded based on windows and forecast horizons for more pratical use.\n",
    "\n",
    "For this particular exercise we will only be focusing on predicting for the next hour given the values of the current hour. As such we need to start by establishing the baseline, i.e.: what are we going to compare our model agaisnt.\n",
    "\n",
    "To make it simple, let's just use the value of the price at a given time and evaluate its Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) against the next hour which will be our baseline. To be clear, our evaluation metric will be RMSE, I will be including MAE just to have a better grasp at how the model is evolving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae0dc0-fc93-4cd9-99ba-46fe4ec2f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "baseline_predictions = energy_data['price actual'].shift(-1).dropna()\n",
    "baseline_rmse = np.sqrt(mean_squared_error(energy_data['price actual'][:-1], baseline_predictions))\n",
    "baseline_mae = mean_absolute_error(energy_data['price actual'][:-1], baseline_predictions)\n",
    "\n",
    "print(f'RMSE: {baseline_rmse:.2f}')\n",
    "print(f'MAE: {baseline_mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f969da-a670-4222-ab30-d4623d692501",
   "metadata": {},
   "source": [
    "Our goal now is to create a deep learning model that can beat 3.40 RMSE. Let's start by preparing the data by adding the lag features and other contextual time data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00505a32-1180-4f22-9ccd-9d84e82faadd",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6540042-c345-4316-b9df-e4bb739a0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = energy_data[numerical + ['time']].copy()\n",
    "\n",
    "# Create contextual features from the date\n",
    "model_data['year'] = model_data['time'].dt.year\n",
    "model_data['month'] = model_data['time'].dt.month\n",
    "model_data['day'] = model_data['time'].dt.day\n",
    "model_data['hour'] = model_data['time'].dt.hour\n",
    "model_data['dayofweek'] = model_data['time'].dt.dayofweek\n",
    "\n",
    "# Create lag features\n",
    "model_data['price_actual_lag24h'] = model_data['price actual'].shift(24)\n",
    "model_data['price_actual_lag12h'] = model_data['price actual'].shift(12)\n",
    "model_data['price_actual_lag6h'] = model_data['price actual'].shift(6)\n",
    "model_data['price_actual_lag3h'] = model_data['price actual'].shift(3)\n",
    "model_data['price_actual_lag1h'] = model_data['price actual'].shift(1)\n",
    "\n",
    "# We need to drop the nulls that were introducted with the shifts\n",
    "model_data = model_data.dropna(how='any', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1e8d6-9ff7-463b-8f08-d2b4cfd20990",
   "metadata": {},
   "source": [
    "Next, let's split the data in training and test data and scale the data as per our previous analysis. Let's use 18 months for training and leave 6 months the end for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d6e3c-f125-4ee6-a2a6-0a9e49fc21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = model_data[(model_data['time'] >= '2016-01-01') & (model_data['time'] < '2017-06-01')]\n",
    "test_data = model_data[model_data['time'] >= '2017-06-01']\n",
    "\n",
    "timestamps_train = train_data['time']\n",
    "timestamps_test = test_data['time']\n",
    "\n",
    "# Separate features and target for train and test\n",
    "# Note we are including 'price actual' in the dataset because our target variable is actually the shift by 1h of this variable\n",
    "X_train = train_data.drop(columns=['time'])[:-1]\n",
    "y_train = train_data['price actual'].shift(-1).dropna()\n",
    "\n",
    "X_test = test_data.drop(columns=['time'])[:-1]\n",
    "y_test = test_data['price actual'].shift(-1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e409b5d-76a9-4033-aac5-6323dbec1a2e",
   "metadata": {},
   "source": [
    "Finally, let's normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43812c26-4153-4143-a100-32e213004f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the input data for LSTM (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fb369-8798-4edd-8208-354340bc4666",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732118ce-a79f-41da-ac5e-5ef759b98229",
   "metadata": {},
   "source": [
    "When dealing with timeseries there are two main deep learning models that should be considered: Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM).\n",
    "\n",
    "As such, I will start our model search by testing single layered models of RNNs, LSTM and 1D Convolutional neural network as well as stacked versions of those - stacked RNN, stacked LSTM and CNN with both RNN and LSTM. In total I will test try with 9 models:\n",
    "* Single LSTM layer\n",
    "* Single RNN layer\n",
    "* Single 1D Convolutional (CNN) layer\n",
    "* Stacked RNN layer\n",
    "* Stacked LSTM layer\n",
    "* CNN with an addittional LSTM layer\n",
    "* CNN with an addittional RNN layer\n",
    "* \n",
    "I will start with ChatGPT suggested sizes for each and then fine-tune the best one.\n",
    "\n",
    "For each model I will test several drop levels (50%, 10% and 0%) to see the effect of drop on each of the models.\n",
    "\n",
    "Let's then start by importing tensorflow and keras and creating these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508df33-e97a-4216-b74e-ccae3f027a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "keras.utils.set_random_seed(812)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0df02-b400-4108-9cb5-40397f1ce6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, Flatten, SimpleRNN\n",
    "\n",
    "# Model 1: simple LSTM\n",
    "def simple_lstm_builder(dropout_rate):\n",
    "    simple_lstm = Sequential()\n",
    "    simple_lstm.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    simple_lstm.add(Dropout(dropout_rate))\n",
    "    simple_lstm.add(Dense(1))\n",
    "    simple_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return simple_lstm\n",
    "\n",
    "# Model 2: simple RNN\n",
    "def simple_rnn_builder(dropout_rate):\n",
    "    simple_rnn = Sequential()\n",
    "    simple_rnn.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    simple_rnn.add(Dropout(dropout_rate))\n",
    "    simple_rnn.add(Dense(1))\n",
    "    simple_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return simple_rnn\n",
    "\n",
    "# Model 3: simple CNN\n",
    "def simple_cnn_builder(dropout_rate):\n",
    "    simple_cnn = Sequential()\n",
    "    simple_cnn.add(Conv1D(filters=48, kernel_size=2, activation='relu', padding='same', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    simple_cnn.add(Flatten())\n",
    "    simple_cnn.add(Dropout(dropout_rate))\n",
    "    simple_cnn.add(Dense(1))\n",
    "    simple_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return simple_cnn\n",
    "\n",
    "# Model 4: Dual LSTM\n",
    "def dual_lstm_builder(dropout_rate):\n",
    "    dual_lstm = Sequential()\n",
    "    dual_lstm.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    dual_lstm.add(LSTM(25))\n",
    "    dual_lstm.add(Dropout(dropout_rate))\n",
    "    dual_lstm.add(Dense(1))\n",
    "    dual_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return dual_lstm\n",
    "\n",
    "# Model 5: Dual RNN\n",
    "def dual_rnn_builder(dropout_rate):\n",
    "    dual_rnn = Sequential()\n",
    "    dual_rnn.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    dual_rnn.add(SimpleRNN(25))\n",
    "    dual_rnn.add(Dropout(dropout_rate))\n",
    "    dual_rnn.add(Dense(1))\n",
    "    dual_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return dual_rnn\n",
    "\n",
    "# Model 6: CNN + LSTM\n",
    "def cnn_lstm_builder(dropout_rate):\n",
    "    cnn_lstm = Sequential()\n",
    "    cnn_lstm.add(Conv1D(filters=48, kernel_size=2, activation='relu', padding='same', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    cnn_lstm.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    cnn_lstm.add(Flatten())\n",
    "    cnn_lstm.add(Dropout(dropout_rate))\n",
    "    cnn_lstm.add(Dense(1))\n",
    "    cnn_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return cnn_lstm\n",
    "\n",
    "# Model 7: CNN + RNN\n",
    "def cnn_rnn_builder(dropout_rate):\n",
    "    cnn_rnn = Sequential()\n",
    "    cnn_rnn.add(Conv1D(filters=48, kernel_size=2, activation='relu', padding='same', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    cnn_rnn.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    cnn_rnn.add(Flatten())\n",
    "    cnn_rnn.add(Dropout(dropout_rate))\n",
    "    cnn_rnn.add(Dense(1))\n",
    "    cnn_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return cnn_rnn\n",
    "\n",
    "# Model 8: LSTM + RNN\n",
    "def lstm_rnn_builder(dropout_rate):\n",
    "    lstm_rnn = Sequential()\n",
    "    lstm_rnn.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    lstm_rnn.add(SimpleRNN(25, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    lstm_rnn.add(Dropout(dropout_rate))\n",
    "    lstm_rnn.add(Dense(1))\n",
    "    lstm_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return lstm_rnn\n",
    "\n",
    "# Model 9: RNN + LSTM\n",
    "def rnn_lstm_builder(dropout_rate):\n",
    "    rnn_lstm = Sequential()\n",
    "    rnn_lstm.add(SimpleRNN(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    rnn_lstm.add(LSTM(25, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    rnn_lstm.add(Flatten())\n",
    "    rnn_lstm.add(Dropout(dropout_rate))\n",
    "    rnn_lstm.add(Dense(1))\n",
    "    rnn_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return rnn_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafeae5-f031-4b0b-afec-706a84c811f4",
   "metadata": {},
   "source": [
    "Great, so now let's instantiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d9c56-21f7-4717-b4c9-e27efd212bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.5, 0.1, 0]\n",
    "\n",
    "base_models = [\n",
    "    (\"Simple LSTM\", simple_lstm_builder), \n",
    "    (\"Simple RNN\", simple_rnn_builder), \n",
    "    (\"Simple CNN\", simple_cnn_builder),\n",
    "    (\"Dual LSTM\", dual_lstm_builder),\n",
    "    (\"Dual RNN\", dual_rnn_builder),\n",
    "    (\"CNN + LSTM\", cnn_lstm_builder),\n",
    "    (\"CNN + RNN\", cnn_rnn_builder),\n",
    "    (\"LSTM + RNN\", lstm_rnn_builder),\n",
    "    (\"RNN + LSTM\", rnn_lstm_builder)]\n",
    "\n",
    "model_instances = []\n",
    "\n",
    "for model_name, model in base_models:\n",
    "    for dropout_rate in dropout_rates:\n",
    "         model_instances.append((f'{model_name} @ {dropout_rate:.2f} dropout', model(dropout_rate), dropout_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aca725-3b31-4801-8966-425a3921ab0e",
   "metadata": {},
   "source": [
    "Ok so now let's compute!\n",
    "\n",
    "#### (!) Attention code below is computational heavy (!) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0606446-3573-42fa-b633-c02914a6ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = [tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 3)]\n",
    "epochs = 20\n",
    "\n",
    "model_results = []\n",
    "\n",
    "\n",
    "for model_name, model, dropout_rate in model_instances:\n",
    "    print(f'### Training: {model_name}')\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks = early_stopping, verbose=0)\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "    final_train_loss = training_loss[-1]\n",
    "    final_val_loss = validation_loss[-1]\n",
    "    epoch_num = len(history.history[\"loss\"])\n",
    "    print(f'Finished training after {epoch_num} epochs')\n",
    "    print(f'Final training loss: {final_train_loss:.2f} | Final validation loss {final_train_loss:.2f}')\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    print(f'RMSE: {rmse:.2f} | MAE: {mae:.2f}')\n",
    "    model_results.append((model_name, history, rmse, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa6f47-0331-4cef-9b43-9f338cfc5968",
   "metadata": {},
   "source": [
    "Ok, so despite the early stopping all models went up to 20 epochs which tells us that maybe there would be a bit more room to extract improvements with further epochs. \n",
    "\n",
    "Let's have a look at how the results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2d7b6-b061-45dc-b6c3-3e35b104b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x9 subplot grid\n",
    "fig, axes = plt.subplots(9, 3, figsize=(15, 30), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the 3x9 grid to iterate through each subplot\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, _, _, _) in enumerate(model_results):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot training and validation loss curves\n",
    "    history = next(history for name, history, _, _ in model_results if name == model_name)\n",
    "    sns.lineplot(x=range(1, len(history.history['loss']) + 1), y=history.history['loss'], label='Training', ax=ax)\n",
    "    sns.lineplot(x=range(1, len(history.history['val_loss']) + 1), y=history.history['val_loss'], label='Validation', ax=ax)\n",
    "    \n",
    "    # Add a text box with RMSE and MAE results\n",
    "    rmse, mae = next((rmse, mae) for name, _, rmse, mae in model_results if name == model_name)\n",
    "    textstr = f'RMSE: {rmse:.2f}\\nMAE: {mae:.2f}'\n",
    "    ax.text(0.5, 0.5, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='center', horizontalalignment='center',\n",
    "            bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Set subplot title\n",
    "    ax.set_title(model_name)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8ac52-ba14-4d1e-86f7-a1034fa0b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model names and corresponding RMSE values\n",
    "model_names = [name for name, _, _, _ in model_results]\n",
    "rmse_values = [rmse for _, _, rmse, _ in model_results]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=rmse_values, y=model_names, palette='pastel')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Model')\n",
    "plt.title('RMSE Comparison Across Models')\n",
    "\n",
    "# Add RMSE values at the end of each bar\n",
    "for i, value in enumerate(rmse_values):\n",
    "    plt.text(value, i, f'{value:.2f}', va='center', ha='left', fontsize=8)\n",
    "\n",
    "# Add a benchmark line\n",
    "plt.axvline(3.40, color='red', linestyle='--', linewidth=2, label='Benchmark = 3.40')\n",
    "\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0b0be-b13d-47e5-b95d-a16e9e7db5cd",
   "metadata": {},
   "source": [
    "NOTE: if you re-run the the code you might get different results, so you'll need to assume the narrative is correct (check git for my output at the moment of writing)\n",
    "\n",
    "Great news - we could already beat out benchmark with a couple of models.\n",
    "\n",
    "Overall, dropout rate doesn't seem to add much except on a few exceptional cases where the 10% dropout beats the model without dropout.\n",
    "\n",
    "Let's have a detailed look at each one:\n",
    "* *Simple LSTM*: Model doesn't beat the benchmark.\n",
    "* *Simple RNN*: Model doesn't beat the benchmark. One of the few cases where small dropout beats without dropout.\n",
    "* *Simple CNN*: Convolutional layer by itself doesn't solve well for this problem - standalone LSTM and RNN work best.\n",
    "* *Dual LSTM*: Adding another LSTM layer doesn't add that much performance vs. single LSTM layer.\n",
    "* *Dual RNN*: Bingo! Adding the inner RNN layer and removing the dropout beats the benchmark! Best performance registered.\n",
    "* *CNN + LSMT*: Second best performing model and already beats the benchmark with 10% dropout and also without drop. Solid architectural choice.\n",
    "* *CNN + RNN*: Curiously adding an RNN layer to the CNN, contraty to the LSTM, doesn't improve performance.\n",
    "* *LSTM + RNN*: Adding an RNN layer layer to an initial LSTM layer seems to lead to good results and can even beat our benchmark without dropout.\n",
    "* *RNN + LSTM*: The contrary (adding an LSTM layer to an RNN layer) doesn't seem to improve single RNN performance and doesn't beat the benchmark.\n",
    "\n",
    "As a next step, let's try to squeeze some additional performance out of the best performing architcture - the Dual RNN layer - with a some fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1db5ed-bb87-4feb-a2e8-68f61d0030fe",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7241371-df18-4e74-b5c1-c4da97e23744",
   "metadata": {},
   "source": [
    "So we are going to fine-tune the Dual RNN model.\n",
    "\n",
    "In our first attempt we started with a first layer of 50 units and an inner one of 25.\n",
    "\n",
    "We are going to test with different layer sizes and optimize for learning rates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fb9f3-abd8-4391-85bb-52ffd6f4c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def dual_rnn_fine_tune_builder(outter_layer_size, inner_layer_size, learning_rate=0.001):\n",
    "    dual_rnn = Sequential()\n",
    "    dual_rnn.add(SimpleRNN(outter_layer_size, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    dual_rnn.add(SimpleRNN(inner_layer_size))\n",
    "    dual_rnn.add(Dropout(dropout_rate))\n",
    "    dual_rnn.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    dual_rnn.compile(optimizer, loss='mean_squared_error')\n",
    "    return dual_rnn\n",
    "\n",
    "outter_layer_sizes = [25, 50, 100]\n",
    "inner_layer_sizes = [15, 25, 50]\n",
    "\n",
    "dual_rnn_instances = []\n",
    "\n",
    "for outter_layer_size in outter_layer_sizes:\n",
    "    for inner_layer_size in inner_layer_sizes:\n",
    "        dual_rnn_instances.append((f'Dual RNN with {outter_layer_size} outter layer and {inner_layer_size} inner layer @ default learning rate', \n",
    "                                   dual_rnn_fine_tune_builder(outter_layer_size, inner_layer_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011283e6-b997-4eef-ad78-169c26d76ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = [tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 3)]\n",
    "epochs = 50\n",
    "\n",
    "fine_tune_results = []\n",
    "\n",
    "for model_name, model in dual_rnn_instances:\n",
    "    print(f'### Training: {model_name}')\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks = early_stopping, verbose=0)\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "    final_train_loss = training_loss[-1]\n",
    "    final_val_loss = validation_loss[-1]\n",
    "    epoch_num = len(history.history[\"loss\"])\n",
    "    print(f'Finished training after {epoch_num} epochs')\n",
    "    print(f'Final training loss: {final_train_loss:.2f} | Final validation loss {final_train_loss:.2f}')\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    print(f'RMSE: {rmse:.2f} | MAE: {mae:.2f}')\n",
    "    fine_tune_results.append((model_name, history, rmse, mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c97cce-7c7c-4024-bdab-2de486fcb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model names and corresponding RMSE values\n",
    "model_names = [name for name, _, _, _ in fine_tune_results]\n",
    "rmse_values = [rmse for _, _, rmse, _ in fine_tune_results]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=rmse_values, y=model_names, palette='pastel')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Model')\n",
    "plt.title('RMSE Comparison Across Models')\n",
    "\n",
    "# Add RMSE values at the end of each bar\n",
    "for i, value in enumerate(rmse_values):\n",
    "    plt.text(value, i, f'{value:.2f}', va='center', ha='left', fontsize=8)\n",
    "\n",
    "# Add a benchmark line\n",
    "plt.axvline(3.40, color='red', linestyle='--', linewidth=2, label='Benchmark = 3.40')\n",
    "plt.axvline(3.06, color='orange', linestyle='--', linewidth=2, label='Best so far = 3.06')\n",
    "\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d50f8-90cc-4ef2-b20c-8dfdbba84a1d",
   "metadata": {},
   "source": [
    "Dual RNN layer without dropout is consistently better than the established benchmark.\n",
    "\n",
    "A larger outter layer with a small inner layer seems be yield the best results and we can beat our previous record with several configurations.\n",
    "\n",
    "Best result seems to be with 100 outter layer and 25 inner layer, so before we set ourselves on that solution, let's just try with yet an additional inner layer of 15 since the results with a 15 layer also showed promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158d99b-ec8d-43d0-923d-b0ef859dd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_triple_rnn = Sequential()\n",
    "deep_triple_rnn.add(SimpleRNN(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "deep_triple_rnn.add(SimpleRNN(25, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "deep_triple_rnn.add(SimpleRNN(15, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "deep_triple_rnn.add(Flatten())\n",
    "deep_triple_rnn.add(Dense(1))\n",
    "deep_triple_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "deep_triple_rnn.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks = early_stopping, verbose=1)\n",
    "deep_triple_rnn_predictions = deep_triple_rnn.predict(X_test, verbose=1)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, deep_triple_rnn_predictions))\n",
    "mae = mean_absolute_error(y_test, deep_triple_rnn_predictions)\n",
    "print(f'RMSE: {rmse:.2f} | MAE: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806641b-ff97-4072-8139-30fe337d6d5e",
   "metadata": {},
   "source": [
    "Seems like adding yet another extra layer doesn't help so let's keep our best configuration so far: 100 outter layer and 25 inner layer and let's just try to optimize the learning rate to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b23b3-ea7c-4498-aa67-3d7887b18cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "dual_rnn_100_25_instances = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    dual_rnn_100_25_instances.append((f'Training Dual RNN with 100 outter layer and 25 inner layer @ {learning_rate:.4f} learning rate',\n",
    "                                      dual_rnn_fine_tune_builder(100, 25, learning_rate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03576b3a-0abc-4ea4-a964-45e579dcb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = [tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 3)]\n",
    "epochs = 50\n",
    "\n",
    "fine_tune_results = []\n",
    "\n",
    "for model_name, model in dual_rnn_100_25_instances:\n",
    "    print(f'### Training: {model_name}')\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks = early_stopping, verbose=0)\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "    final_train_loss = training_loss[-1]\n",
    "    final_val_loss = validation_loss[-1]\n",
    "    epoch_num = len(history.history[\"loss\"])\n",
    "    print(f'Finished training after {epoch_num} epochs')\n",
    "    print(f'Final training loss: {final_train_loss:.2f} | Final validation loss {final_train_loss:.2f}')\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    print(f'RMSE: {rmse:.2f} | MAE: {mae:.2f}')\n",
    "    fine_tune_results.append((model_name, history, rmse, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ddfc7-97c6-49f6-bb0b-e80a35b92711",
   "metadata": {},
   "source": [
    "Ok, changing the learning rate doesn't seem to help, so let's just call it the best model a Stacked Recurrent Neural Network with 100 outter layer and 25 inner layer with a learning rate of 0.001 and let's optimize for epochs to prepare the training script.\n",
    "\n",
    "Let's let it run for 150 epochs and create a check-point for the minimum loss so that we can then use it as our model in production instead of training a new one that might come with a differnt peformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb933d2-c8df-4ec5-bec1-9ac2cba5dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = dual_rnn_fine_tune_builder(100,25,0.001)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'checkpoints/best-model_v1_{epoch:02d}_{val_loss:02f}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "selected_model.fit(X_train, y_train, epochs=150, validation_data=(X_test, y_test), callbacks = checkpoint, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f82b60-f23e-4892-957a-adf533213f2a",
   "metadata": {},
   "source": [
    "Let's just double-check how the final model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac204b-750a-43f0-9ed1-6c64b3d10d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = keras.models.load_model('checkpoints/best-model_v1_36_8.344853.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69259fd-ab73-4987-a626-f0892388ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_predictions = final_model.predict(X_test, verbose=0)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_model_predictions))\n",
    "mae = mean_absolute_error(y_test, final_model_predictions)\n",
    "print(f'RMSE: {rmse:.2f} | MAE: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4f8a9-0ec3-4759-b71e-bb7289367972",
   "metadata": {},
   "source": [
    "Not the bet performance we got historically with this model, but still a very good one - better than the benchmark and better than all the models trained at first, which means that we could improve based on fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a2d8d-0bd5-4af2-a21e-def32edc2ba6",
   "metadata": {},
   "source": [
    "## Conclusion and results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932622f-97da-4793-a550-000eadfc52de",
   "metadata": {},
   "source": [
    "Ok, so we proved that we are able to better predict the price of electricity for the next hour with a neural network than simply guessing it based on the previous hour.\n",
    "\n",
    "For that, we've build an Stacked Recurrent Neural Network, with an outter layer of 100 and an additional inner layer of 25. Adam with 0.001 learning rate seems to be the best optimizer.\n",
    "\n",
    "For our training script we will be using the weights of the best checkpoint from a training up to 150 epochs.\n",
    "\n",
    "Below you can see how 2 weeks of actual data compares with our projections alongside with the naive predictor that we've started with (previous hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93288e96-e18c-463e-a313-49b72ac2a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_points = 24*7\n",
    "\n",
    "# First subplot: Actual values and naive approach\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.arange(num_data_points), y_test[:num_data_points], label='Actual Values')\n",
    "plt.plot(np.arange(num_data_points), y_test[:num_data_points].shift(1), label='Naive Approach (Previous Hour Values)')\n",
    "plt.title('Actual Values and Naive Approach for the First 7 Days')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot: Model predictions and actual values\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.arange(num_data_points), y_test[:num_data_points], label='Actual Values')\n",
    "plt.plot(np.arange(num_data_points), final_model_predictions[:num_data_points], label='Model Predictions')\n",
    "plt.title('Model Predictions vs Actual Values for the First 7 Days')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbea9b5-5e86-45eb-b8b1-883d9dd6a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
